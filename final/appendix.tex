\begin{appendices}
    \section*{Appendix}

    % \section{Feature Importance, Feed-Forward Neural Network}
    %     \begin{figure}[h!]
    %         \centering
    %         \includegraphics[scale=0.4]{"./images/feature_importance.jpg"}
    %         \caption{Features plotted in order of importance, ascending.}
    %     \end{figure}
    % \pagebreak

    \section{Description of Fields in Time-Series Dataset}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|}
        \hline
        \textbf{Field} & \textbf{Description} \\
        \hline
        step & Step count \\
        X & X-axis acceleration of the heartrate monitor \\
        Y & Y-axis acceleration of the heartrate monitor \\
        Z & Z-axis acceleration of the heartrate monitor \\
        enmo & Euclidean Norm Minus One (ENMO) \\
        anglez & Angle in the Z-axis \\
        non-wear\_flag & Non-wear flag \\
        light & Light exposure \\
        battery\_voltage & Battery voltage of the monitor \\
        time\_of\_day & Time of day \\
        weekday & Day of the week \\
        quarter & Quarter of the year \\
        relative\_date\_PCIAT & Current PCIAT minus previous day PCIAT \\
        \hline
        \end{tabular}
        % \caption{Description of fields in the time-series dataset}
        \label{table:fields}
    \end{table}

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{"./images/feature_importance.jpg"}
        \caption{Features plotted in order of importance according to the XGBoost classifier, ascending.}
      \end{figure}

      \section{Accuracy of Feed Forward Neural Network on Cross-Sectional Data}
      \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.8]{"./images/model_accuracy.jpg"}
        \caption{Model accuracy of Feed-Forward Neural Network}
    \end{figure}

    % \section{Kaggle Starter Code \cite{antonina_dolgorukova_2024}}
    % \begin{mdframed}
    % \begin{lstlisting}[breaklines=true]
    % # Starter Notebook: Multi-Target Prediction Using CatBoost

    % See [this discussion](https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/discussion/535121) for more information.
    % import warnings
    % from functools import partial
    % from pathlib import Path

    % import matplotlib.pyplot as plt
    % import numpy as np
    % import optuna
    % import polars as pl
    % import polars.selectors as cs
    % from catboost import CatBoostRegressor, MultiTargetCustomMetric
    % from numpy.typing import ArrayLike, NDArray
    % from polars.testing import assert_frame_equal
    % from sklearn.base import BaseEstimator
    % from sklearn.metrics import cohen_kappa_score
    % from sklearn.model_selection import StratifiedKFold

    % warnings.filterwarnings("ignore", message="Failed to optimize method")

    % DATA_DIR = Path("./child-mind-institute-problematic-internet-use")
    % TARGET_COLS = [
    %     "PCIAT-PCIAT_01",
    %     "PCIAT-PCIAT_02",
    %     "PCIAT-PCIAT_03",
    %     "PCIAT-PCIAT_04",
    %     "PCIAT-PCIAT_05",
    %     "PCIAT-PCIAT_06",
    %     "PCIAT-PCIAT_07",
    %     "PCIAT-PCIAT_08",
    %     "PCIAT-PCIAT_09",
    %     "PCIAT-PCIAT_10",
    %     "PCIAT-PCIAT_11",
    %     "PCIAT-PCIAT_12",
    %     "PCIAT-PCIAT_13",
    %     "PCIAT-PCIAT_14",
    %     "PCIAT-PCIAT_15",
    %     "PCIAT-PCIAT_16",
    %     "PCIAT-PCIAT_17",
    %     "PCIAT-PCIAT_18",
    %     "PCIAT-PCIAT_19",
    %     "PCIAT-PCIAT_20",
    %     "PCIAT-PCIAT_Total",
    %     "sii",
    % ]

    % FEATURE_COLS = [
    %     "Basic_Demos-Enroll_Season",
    %     "Basic_Demos-Age",
    %     "Basic_Demos-Sex",
    %     "CGAS-Season",
    %     "CGAS-CGAS_Score",
    %     "Physical-Season",
    %     "Physical-BMI",
    %     "Physical-Height",
    %     "Physical-Weight",
    %     "Physical-Waist_Circumference",
    %     "Physical-Diastolic_BP",
    %     "Physical-HeartRate",
    %     "Physical-Systolic_BP",
    %     "Fitness_Endurance-Season",
    %     "Fitness_Endurance-Max_Stage",
    %     "Fitness_Endurance-Time_Mins",
    %     "Fitness_Endurance-Time_Sec",
    %     "FGC-Season",
    %     "FGC-FGC_CU",
    %     "FGC-FGC_CU_Zone",
    %     "FGC-FGC_GSND",
    %     "FGC-FGC_GSND_Zone",
    %     "FGC-FGC_GSD",
    %     "FGC-FGC_GSD_Zone",
    %     "FGC-FGC_PU",
    %     "FGC-FGC_PU_Zone",
    %     "FGC-FGC_SRL",
    %     "FGC-FGC_SRL_Zone",
    %     "FGC-FGC_SRR",
    %     "FGC-FGC_SRR_Zone",
    %     "FGC-FGC_TL",
    %     "FGC-FGC_TL_Zone",
    %     "BIA-Season",
    %     "BIA-BIA_Activity_Level_num",
    %     "BIA-BIA_BMC",
    %     "BIA-BIA_BMI",
    %     "BIA-BIA_BMR",
    %     "BIA-BIA_DEE",
    %     "BIA-BIA_ECW",
    %     "BIA-BIA_FFM",
    %     "BIA-BIA_FFMI",
    %     "BIA-BIA_FMI",
    %     "BIA-BIA_Fat",
    %     "BIA-BIA_Frame_num",
    %     "BIA-BIA_ICW",
    %     "BIA-BIA_LDM",
    %     "BIA-BIA_LST",
    %     "BIA-BIA_SMM",
    %     "BIA-BIA_TBW",
    %     "PAQ_A-Season",
    %     "PAQ_A-PAQ_A_Total",
    %     "PAQ_C-Season",
    %     "PAQ_C-PAQ_C_Total",
    %     "SDS-Season",
    %     "SDS-SDS_Total_Raw",
    %     "SDS-SDS_Total_T",
    %     "PreInt_EduHx-Season",
    %     "PreInt_EduHx-computerinternet_hoursday",
    % ]
    % # Load data
    % train = pl.read_csv(DATA_DIR / "train.csv")
    % test = pl.read_csv(DATA_DIR / "test.csv")
    % train_test = pl.concat([train, test], how="diagonal")

    % IS_TEST = test.height <= 100

    % assert_frame_equal(train, train_test[: train.height].select(train.columns))
    % assert_frame_equal(test, train_test[train.height :].select(test.columns))
    % # Cast string columns to categorical
    % train_test = train_test.with_columns(cs.string().cast(pl.Categorical).fill_null("NAN"))
    % train = train_test[: train.height]
    % test = train_test[train.height :]

    % # ignore rows with null values in TARGET_COLS
    % train_without_null = train_test.drop_nulls(subset=TARGET_COLS)
    % X = train_without_null.select(FEATURE_COLS)
    % X_test = test.select(FEATURE_COLS)
    % y = train_without_null.select(TARGET_COLS)
    % y_sii = y.get_column("sii").to_numpy()  # ground truth
    % cat_features = X.select(cs.categorical()).columns
    % class MultiTargetQWK(MultiTargetCustomMetric):
    %     def get_final_error(self, error, weight):
    %         return np.sum(error)  # / np.sum(weight)

    %     def is_max_optimal(self):
    %         # if True, the bigger the better
    %         return True

    %     def evaluate(self, approxes, targets, weight):
    %         approx = np.clip(approxes[-1], 0, 3).round().astype(int)
    %         target = targets[-1]

    %         qwk = cohen_kappa_score(target, approx, weights="quadratic")

    %         return qwk, 1

    %     def get_custom_metric_name(self):
    %         return "MultiTargetQWK"


    % class OptimizedRounder:
    %     """
    %     A class for optimizing the rounding of continuous predictions into discrete class labels using Optuna.
    %     The optimization process maximizes the Quadratic Weighted Kappa score by learning thresholds that separate
    %     continuous predictions into class intervals.

    %     Args:
    %         n_classes (int): The number of discrete class labels.
    %         n_trials (int, optional): The number of trials for the Optuna optimization. Defaults to 100.

    %     Attributes:
    %         n_classes (int): The number of discrete class labels.
    %         labels (NDArray[np.int_]): An array of class labels from 0 to `n_classes - 1`.
    %         n_trials (int): The number of optimization trials.
    %         metric (Callable): The Quadratic Weighted Kappa score metric used for optimization.
    %         thresholds (List[float]): The optimized thresholds learned after calling `fit()`.

    %     Methods:
    %         fit(y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:
    %             Fits the rounding thresholds based on continuous predictions and ground truth labels.

    %             Args:
    %                 y_pred (NDArray[np.float_]): Continuous predictions that need to be rounded.
    %                 y_true (NDArray[np.int_]): Ground truth class labels.

    %             Returns:
    %                 None

    %         predict(y_pred: NDArray[np.float_]) -> NDArray[np.int_]:
    %             Predicts discrete class labels by rounding continuous predictions using the fitted thresholds.
    %             `fit()` must be called before `predict()`.

    %             Args:
    %                 y_pred (NDArray[np.float_]): Continuous predictions to be rounded.

    %             Returns:
    %                 NDArray[np.int_]: Predicted class labels.

    %         _normalize(y: NDArray[np.float_]) -> NDArray[np.float_]:
    %             Normalizes the continuous values to the range [0, `n_classes - 1`].

    %             Args:
    %                 y (NDArray[np.float_]): Continuous values to be normalized.

    %             Returns:
    %                 NDArray[np.float_]: Normalized values.

    %     References:
    %         - This implementation uses Optuna for threshold optimization.
    %         - Quadratic Weighted Kappa is used as the evaluation metric.
    %     """

    %     def __init__(self, n_classes: int, n_trials: int = 100):
    %         self.n_classes = n_classes
    %         self.labels = np.arange(n_classes)
    %         self.n_trials = n_trials
    %         self.metric = partial(cohen_kappa_score, weights="quadratic")

    %     def fit(self, y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:
    %         y_pred = self._normalize(y_pred)

    %         def objective(trial: optuna.Trial) -> float:
    %             thresholds = []
    %             for i in range(self.n_classes - 1):
    %                 low = max(thresholds) if i > 0 else min(self.labels)
    %                 high = max(self.labels)
    %                 th = trial.suggest_float(f"threshold_{i}", low, high)
    %                 thresholds.append(th)
    %             try:
    %                 y_pred_rounded = np.digitize(y_pred, thresholds)
    %             except ValueError:
    %                 return -100
    %             return self.metric(y_true, y_pred_rounded)

    %         optuna.logging.disable_default_handler()
    %         study = optuna.create_study(direction="maximize")
    %         study.optimize(
    %             objective,
    %             n_trials=self.n_trials,
    %         )
    %         self.thresholds = [study.best_params[f"threshold_{i}"] for i in range(self.n_classes - 1)]

    %     def predict(self, y_pred: NDArray[np.float_]) -> NDArray[np.int_]:
    %         assert hasattr(self, "thresholds"), "fit() must be called before predict()"
    %         y_pred = self._normalize(y_pred)
    %         return np.digitize(y_pred, self.thresholds)

    %     def _normalize(self, y: NDArray[np.float_]) -> NDArray[np.float_]:
    %         # normalize y_pred to [0, n_classes - 1]
    %         return (y - y.min()) / (y.max() - y.min()) * (self.n_classes - 1)
    % # setting catboost parameters
    % params = dict(
    %     loss_function="MultiRMSE",
    %     eval_metric=MultiTargetQWK(),
    %     iterations=1 if IS_TEST else 100000,
    %     learning_rate=0.1,
    %     depth=5,
    %     early_stopping_rounds=50,
    % )

    % # Cross-validation
    % skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)
    % models: list[CatBoostRegressor] = []
    % y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)
    % for train_idx, val_idx in skf.split(X, y_sii):
    %     X_train: pl.DataFrame
    %     X_val: pl.DataFrame
    %     y_train: pl.DataFrame
    %     y_val: pl.DataFrame
    %     X_train, X_val = X[train_idx], X[val_idx]
    %     y_train, y_val = y[train_idx], y[val_idx]

    %     # train model
    %     model = CatBoostRegressor(**params)
    %     model.fit(
    %         X_train.to_pandas(),
    %         y_train.to_pandas(),
    %         eval_set=(X_val.to_pandas(), y_val.to_pandas()),
    %         cat_features=cat_features,
    %         verbose=False,
    %     )
    %     models.append(model)

    %     # predict
    %     y_pred[val_idx] = model.predict(X_val.to_pandas())

    % assert np.isnan(y_pred).sum() == 0
    % # Optimize thresholds
    % optimizer = OptimizedRounder(n_classes=4, n_trials=300)
    % y_pred_total = y_pred[:, TARGET_COLS.index("PCIAT-PCIAT_Total")]
    % optimizer.fit(y_pred_total, y_sii)
    % y_pred_rounded = optimizer.predict(y_pred_total)

    % # Calculate QWK
    % qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights="quadratic")
    % print(f"Cross-Validated QWK Score: {qwk}")
    % feature_importance = np.mean([model.get_feature_importance() for model in models], axis=0)
    % sorted_idx = np.argsort(feature_importance)
    % fig = plt.figure(figsize=(12, 10))
    % plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align="center")
    % plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])
    % plt.title("Feature Importance")
    % class AvgModel:
    %     def __init__(self, models: list[BaseEstimator]):
    %         self.models = models

    %     def predict(self, X: ArrayLike) -> NDArray[np.int_]:
    %         preds: list[NDArray[np.int_]] = []
    %         for model in self.models:
    %             pred = model.predict(X)
    %             preds.append(pred)

    %         return np.mean(preds, axis=0)
    % avg_model = AvgModel(models)
    % test_pred = avg_model.predict(X_test.to_pandas())[:, TARGET_COLS.index("PCIAT-PCIAT_Total")]
    % test_pred_rounded = optimizer.predict(test_pred)
    % test.select("id").with_columns(
    %     pl.Series("sii", pl.Series("sii", test_pred_rounded)),
    % ).write_csv("submission.csv")
    % \end{lstlisting}
    % \end{mdframed}

    \pagebreak
    \section{Hypothesis Testing}
    In order to compare two models A and B the null hypothesis is that the distribution of $acc(A)_i - acc(B)_i$ has zero mean and the alternative hypothesis is that the model with the higher mean performance accuarcy is significantly better than the other.\\
    T statistic is given by:
        \[
        t = \frac{\overline{acc}(A) - \overline{acc}(B)}{\sqrt{var(A - B)/k}}
        \]
        where,
        \[
        var(A - B) = \frac{1}{k}\sum_{i=1}^k [acc(A)_i - acc(B)_i - (\overline{acc}(A) - \overline{acc}(B))]^2
        \]

        \begin{table}[h!]
            \centering
            \caption{Comparison of Models using t-statistic and p-value}
            \begin{tabular}{|l|l|c|c|l|}
                \hline
                \textbf{Model 1} & \textbf{Model 2} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Accept/Reject Null} \\
                \hline
                RF & XGB & -0.492 & 0.633 & Accept \\
                \hline
                RF & SVC & 6.149 & 0.0 & Reject \\
                \hline
                RF & FFN & -0.039 & 0.970 & Accept \\
                \hline
                XGB & FFN & 0.304 & 0.767 & Accept \\
                \hline
                XGB & SVC & 4.104 & 0.002 & Reject \\
                \hline
                FFN & SVC & 4.588 & 0.001 & Reject \\
                \hline
            \end{tabular}
        \end{table}
    Therefore, we can conclude that RF, XGB and FFN are significantly better than SVC. 
    However it looks like the distribution of accuracies of RF , XGB and FFN are comparable. 
    In this case we will choose the model that requires the least amount of parameters which is the XGBoost classifier.

%     \pagebreak
%     \section{SII Prediction Code}
%     \begin{mdframed}
%     \begin{lstlisting}[breaklines=true]
%         import numpy as np
% import torch
% from torch.utils.data import Dataset, DataLoader
% import pandas as pd
% from sklearn.experimental import enable_iterative_imputer 
% from sklearn.impute import IterativeImputer
% from sklearn.semi_supervised import LabelPropagation
% from sklearn.preprocessing import StandardScaler
% import torch.nn as nn
% import torch.optim as optim
% import os
% os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

% def preprocess_time_series(parquet_dir, output_dir, batch_size=100):
%     os.makedirs(output_dir, exist_ok=True)

%     # Traverse subdirectories and collect parquet files
%     patient_dirs = [d for d in os.listdir(parquet_dir) if os.path.isdir(os.path.join(parquet_dir, d))]

%     for i in range(0, len(patient_dirs), batch_size):
%         batch_dirs = patient_dirs[i:i + batch_size]
        
%         for patient_dir in batch_dirs:
%             patient_id = patient_dir.split('=')[-1]  # Extract patient ID from the folder name
%             patient_path = os.path.join(parquet_dir, patient_dir, 'part-0.parquet')  # Path to the parquet file
            
%             if os.path.exists(patient_path):
%                 # Load and process time-series data
%                 df = pd.read_parquet(patient_path)
                
%                 # Specify the time-series features to extract
%                 time_series_features = ['X', 'Y', 'Z', 'enmo', 'anglez', 'non-wear_flag', 'light', 'battery_voltage']
                
%                 # Filter the dataframe for relevant features
%                 df = df[time_series_features]
                
%                 # Convert to numpy array (with float32 for memory efficiency)
%                 ts_data = df.to_numpy(dtype=np.float32)
                
%                 # Save processed data as .npy file
%                 np.save(os.path.join(output_dir, f"{patient_id}.npy"), ts_data)


% # Data Preprocessing
% def data_pre_processing(file_name):
%     df = pd.read_csv(file_name)
%     TARGET_COLS = ["sii"]
    
%     FEATURE_COLS = [
%         "Basic_Demos-Age",
%         "Basic_Demos-Sex",
%         "CGAS-CGAS_Score",
%         "Physical-BMI",
%         "Physical-Height",
%         "Physical-Weight",
%         "Physical-Waist_Circumference",
%         "Physical-Diastolic_BP",
%         "Physical-HeartRate",
%         "Physical-Systolic_BP",
%         "Fitness_Endurance-Max_Stage",
%         "Fitness_Endurance-Time_Mins",
%         "Fitness_Endurance-Time_Sec",
%         "FGC-FGC_CU",
%         "FGC-FGC_CU_Zone",
%         "FGC-FGC_GSND",
%         "FGC-FGC_GSND_Zone",
%         "FGC-FGC_GSD",
%         "FGC-FGC_GSD_Zone",
%         "FGC-FGC_PU",
%         "FGC-FGC_PU_Zone",
%         "FGC-FGC_SRL",
%         "FGC-FGC_SRL_Zone",
%         "FGC-FGC_SRR",
%         "FGC-FGC_SRR_Zone",
%         "FGC-FGC_TL",
%         "FGC-FGC_TL_Zone",
%         "BIA-BIA_Activity_Level_num",
%         "BIA-BIA_BMC",
%         "BIA-BIA_BMI",
%         "BIA-BIA_BMR",
%         "BIA-BIA_DEE",
%         "BIA-BIA_ECW",
%         "BIA-BIA_FFM",
%         "BIA-BIA_FFMI",
%         "BIA-BIA_FMI",
%         "BIA-BIA_Fat",
%         "BIA-BIA_Frame_num",
%         "BIA-BIA_ICW",
%         "BIA-BIA_LDM",
%         "BIA-BIA_LST",
%         "BIA-BIA_SMM",
%         "BIA-BIA_TBW",
%         "PAQ_A-PAQ_A_Total",
%         "PAQ_C-PAQ_C_Total",
%         "SDS-SDS_Total_Raw",
%         "SDS-SDS_Total_T",
%         "PreInt_EduHx-computerinternet_hoursday"]

%     data = df[FEATURE_COLS]
%     target = df[TARGET_COLS].fillna(-1).values.flatten()
%     patient_ids = df["id"]

%     iterative_imputer = IterativeImputer(max_iter=10, random_state=0)
%     data_imputed = pd.DataFrame(iterative_imputer.fit_transform(data), columns=FEATURE_COLS)

%     scaler = StandardScaler()
%     data_scaled = scaler.fit_transform(data_imputed)

%     label_prop_model = LabelPropagation(kernel='knn', n_neighbors=5)
%     label_prop_model.fit(data_scaled, target)
%     target_imputed = label_prop_model.transduction_

%     return pd.DataFrame(data_scaled, columns=FEATURE_COLS), target_imputed, patient_ids


% # Time-Series Dataset
% # class LazyPatientDataset(Dataset):
% #     def __init__(self, patient_ids, static_features, labels, ts_dir, pad_value=0.0):
% #         self.patient_ids = patient_ids
% #         self.static_features = static_features
% #         self.labels = labels
% #         self.ts_dir = ts_dir
% #         self.pad_value = pad_value

% #     def __len__(self):
% #         return len(self.labels)

% #     def __getitem__(self, idx):
% #         patient_id = self.patient_ids[idx]
% #         static_feat = torch.tensor(self.static_features.iloc[idx].to_numpy(), dtype=torch.float32)
% #         label = torch.tensor(self.labels[idx], dtype=torch.long)

% #         ts_filepath = os.path.join(self.ts_dir, f"{patient_id}.npy")
% #         if os.path.exists(ts_filepath):
% #             time_series = torch.tensor(np.load(ts_filepath), dtype=torch.float32)
% #         else:
% #             time_series = None

% #         return time_series, static_feat, label

% class LazyPatientDataset(Dataset):
%     def __init__(self, patient_ids, static_features, labels, ts_dir, pad_value=0.0):
%         self.patient_ids = patient_ids
%         self.static_features = static_features
%         self.labels = labels
%         self.ts_dir = ts_dir
%         self.pad_value = pad_value

%     def __len__(self):
%         return len(self.labels)

%     def __getitem__(self, idx):
%         patient_id = self.patient_ids[idx]
%         static_feat = torch.tensor(self.static_features.iloc[idx].to_numpy(), dtype=torch.float32)
%         label = torch.tensor(self.labels[idx], dtype=torch.long)

%         ts_filepath = os.path.join(self.ts_dir, f"{patient_id}.npy")
%         if os.path.exists(ts_filepath):
%             time_series = torch.tensor(np.load(ts_filepath), dtype=torch.float32)
%         else:
%             time_series = None

%         return time_series, static_feat, label  # Return only 3 elements

    
% def collate_fn(batch):
%     time_series, static_features, labels = zip(*batch)  # Unpack into 3 variables

%     # Filter out None time-series entries and their corresponding static features and labels
%     valid_indices = [i for i, ts in enumerate(time_series) if ts is not None]
%     valid_time_series = [time_series[i] for i in valid_indices]
%     static_features = [static_features[i] for i in valid_indices]
%     labels = [labels[i] for i in valid_indices]

%     # Pad the time series if there are valid ones
%     if valid_time_series:
%         time_series_padded = pad_sequence(valid_time_series, batch_first=True, padding_value=0)
        
%         # Create mask: True for non-zero elements along the last dimension
%         ts_masks = (time_series_padded != 0).any(dim=-1).float()
%     else:
%         # Handle empty batch gracefully
%         time_series_padded = torch.zeros(len(batch), 1, valid_time_series[0].size(-1))
%         ts_masks = torch.zeros(len(batch), 1, dtype=torch.float)

%     # Stack static features and labels
%     static_features = torch.stack(static_features, dim=0)
%     labels = torch.tensor(labels, dtype=torch.long)

%     return time_series_padded, static_features, labels, ts_masks


% class TimeSeriesTransformer(nn.Module):
%     def __init__(self, input_dim, embed_dim, num_heads, num_layers, max_len):
%         super(TimeSeriesTransformer, self).__init__()
%         self.input_projection = nn.Linear(input_dim, embed_dim) if input_dim != embed_dim else nn.Identity()
%         self.positional_encoding = nn.Parameter(torch.randn(1, max_len, embed_dim))  # Shape: [1, max_len, embed_dim]
        
%         self.encoder = nn.TransformerEncoder(
%             nn.TransformerEncoderLayer(embed_dim, num_heads, batch_first = True),
%             num_layers,
%         )
%         self.fc = nn.Linear(embed_dim, embed_dim)
%     def forward(self, x, mask=None):
%         # x: [batch_size, seq_len, input_dim]
%         seq_len = x.size(1)
    
%         # Project to embedding if necessary
%         x = self.input_projection(x)  # [batch_size, seq_len, embed_dim]
    
%         # Add positional encoding for the current sequence length
%         x = x + self.positional_encoding[:, :seq_len, :]  # [batch_size, seq_len, embed_dim]
    
%         # Transformer Encoder
%         x = self.encoder(x, src_key_padding_mask=mask)  # [batch_size, seq_len, embed_dim]
    
%         # Pool over time (mean pooling)
%         x = x.mean(dim=1)  # [batch_size, embed_dim]
    
%         return self.fc(x)  # [batch_size, embed_dim]



% class StaticFeatureEmbedder(nn.Module):
%     def __init__(self, input_dim, embed_dim):
%         super().__init__()
%         self.fc = nn.Sequential(
%             nn.Linear(input_dim, embed_dim),
%             nn.ReLU(),
%             nn.Linear(embed_dim, embed_dim)
%         )

%     def forward(self, x):
%         return self.fc(x)


% class FeedForwardClassifier(nn.Module):
%     def __init__(self, input_dim, num_classes):
%         super().__init__()
%         self.model = nn.Sequential(
%             nn.Linear(input_dim, 128),
%             nn.ReLU(),
%             nn.Dropout(0.2),
%             nn.Linear(128, num_classes)
%         )

%     def forward(self, x):
%         return self.model(x)


% class CombinedModel(nn.Module):
%     def __init__(self, time_series_model, static_model, classifier, embed_dim):
%         super().__init__()
%         self.time_series_model = time_series_model
%         self.static_model = static_model
%         self.classifier = classifier
%         self.embed_dim = embed_dim

%     def forward(self, time_series, static_features, mask=None):
%         ts_embedding = self.time_series_model(time_series, mask)
%         static_embedding = self.static_model(static_features)
%         combined = torch.cat([ts_embedding, static_embedding], dim=1)
%         return self.classifier(combined)


% # Training
% def train_model(model, dataloader, criterion, optimizer, device):
%     model.train()
%     total_loss = 0
%     for time_series, static_features, labels, ts_masks in dataloader:
%         time_series, static_features, labels, ts_masks = (
%             time_series.to(device),
%             static_features.to(device),
%             labels.to(device),
%             ts_masks.to(device),
%         )

%         optimizer.zero_grad()
%         outputs = model(time_series, static_features, ts_masks)
%         loss = criterion(outputs, labels)
%         loss.backward()
%         optimizer.step()
%         total_loss += loss.item()

%     return total_loss / len(dataloader)


% if __name__ == "__main__":
%     import os
%     from torch.nn.utils.rnn import pad_sequence

%     # File paths and parameters
%     TS_DIR = "./dataset/series_train.parquet"  # Directory where time-series data is stored
%     CSV_FILE = "./dataset/train.csv"  # Path to CSV file with static features and target
%     PROCESSED_TS_DIR = './dataset/processed_time_series'
    
%     print("Preprocessing time-series data...")
%     preprocess_time_series(TS_DIR, PROCESSED_TS_DIR)
    
%     # Step 1: Preprocess the data
%     print("Pre process static data")
%     static_features, labels, patient_ids = data_pre_processing(CSV_FILE)
    
    
%     BATCH_SIZE = 8
%     EPOCHS = 20
%     LEARNING_RATE = 1e-3
%     DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
%     TIME_SERIES_INPUT_DIM = 8  # Adjust based on your data
%     STATIC_FEATURE_DIM = static_features.shape[1]  # Adjust based on your data
%     EMBED_DIM = 16
%     NUM_CLASSES = 4
%     NUM_HEADS = 4
%     NUM_LAYERS = 2

%     # Step 2: Initialize Dataset and Dataloader
%     dataset = LazyPatientDataset(
%         patient_ids=patient_ids,
%         static_features=static_features,
%         labels=labels,
%         ts_dir=processed_ts_dir,
%     )
%     print("Creating dataset and dataloader...")
%     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

%     # Step 3: Initialize Models
%     time_series_model = TimeSeriesTransformer(
%         input_dim=TIME_SERIES_INPUT_DIM,
%         embed_dim=EMBED_DIM,
%         num_heads=NUM_HEADS,
%         num_layers=NUM_LAYERS,
%         max_len=800000,  # Adjust based on your max sequence length
%     )

%     static_model = StaticFeatureEmbedder(
%         input_dim=STATIC_FEATURE_DIM,
%         embed_dim=EMBED_DIM,
%     )

%     classifier = FeedForwardClassifier(
%         input_dim=2 * EMBED_DIM,
%         num_classes=NUM_CLASSES,
%     )

%     model = CombinedModel(
%         time_series_model=time_series_model,
%         static_model=static_model,
%         classifier=classifier,
%         embed_dim=EMBED_DIM,
%     ).to(DEVICE)

%     # Step 4: Define Optimizer and Loss Function
%     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
%     criterion = nn.CrossEntropyLoss()

%     # Step 5: Train the Model
%     for epoch in range(EPOCHS):
%         loss = train_model(model, dataloader, criterion, optimizer, DEVICE)
%         print(f"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss:.4f}")

%     # Step 6: Save the Model
%     model_path = "./combined_model.pth"
%     torch.save(model.state_dict(), model_path)
%     print(f"Model saved to {model_path}")


%     \end{lstlisting}
%     \end{mdframed}

  
\end{appendices}