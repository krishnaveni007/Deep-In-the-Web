{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72cb59a-1be5-4949-96cf-8497186e7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4b950-4898-4e5b-b905-78d1c6814915",
   "metadata": {},
   "source": [
    "### Preprocess traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8389b308-c9df-4150-856f-b384b5b653b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_processing(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    TARGET_COLS = [\"sii\"]\n",
    "    \n",
    "    FEATURE_COLS = [\n",
    "        \"Basic_Demos-Age\",\n",
    "        \"Basic_Demos-Sex\",\n",
    "        \"CGAS-CGAS_Score\",\n",
    "        \"Physical-BMI\",\n",
    "        \"Physical-Height\",\n",
    "        \"Physical-Weight\",\n",
    "        \"Physical-Waist_Circumference\",\n",
    "        \"Physical-Diastolic_BP\",\n",
    "        \"Physical-HeartRate\",\n",
    "        \"Physical-Systolic_BP\",\n",
    "        \"Fitness_Endurance-Max_Stage\",\n",
    "        \"Fitness_Endurance-Time_Mins\",\n",
    "        \"Fitness_Endurance-Time_Sec\",\n",
    "        \"FGC-FGC_CU\",\n",
    "        \"FGC-FGC_CU_Zone\",\n",
    "        \"FGC-FGC_GSND\",\n",
    "        \"FGC-FGC_GSND_Zone\",\n",
    "        \"FGC-FGC_GSD\",\n",
    "        \"FGC-FGC_GSD_Zone\",\n",
    "        \"FGC-FGC_PU\",\n",
    "        \"FGC-FGC_PU_Zone\",\n",
    "        \"FGC-FGC_SRL\",\n",
    "        \"FGC-FGC_SRL_Zone\",\n",
    "        \"FGC-FGC_SRR\",\n",
    "        \"FGC-FGC_SRR_Zone\",\n",
    "        \"FGC-FGC_TL\",\n",
    "        \"FGC-FGC_TL_Zone\",\n",
    "        \"BIA-BIA_Activity_Level_num\",\n",
    "        \"BIA-BIA_BMC\",\n",
    "        \"BIA-BIA_BMI\",\n",
    "        \"BIA-BIA_BMR\",\n",
    "        \"BIA-BIA_DEE\",\n",
    "        \"BIA-BIA_ECW\",\n",
    "        \"BIA-BIA_FFM\",\n",
    "        \"BIA-BIA_FFMI\",\n",
    "        \"BIA-BIA_FMI\",\n",
    "        \"BIA-BIA_Fat\",\n",
    "        \"BIA-BIA_Frame_num\",\n",
    "        \"BIA-BIA_ICW\",\n",
    "        \"BIA-BIA_LDM\",\n",
    "        \"BIA-BIA_LST\",\n",
    "        \"BIA-BIA_SMM\",\n",
    "        \"BIA-BIA_TBW\",\n",
    "        \"PAQ_A-PAQ_A_Total\",\n",
    "        \"PAQ_C-PAQ_C_Total\",\n",
    "        \"SDS-SDS_Total_Raw\",\n",
    "        \"SDS-SDS_Total_T\",\n",
    "        \"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "\n",
    "    data = df[FEATURE_COLS]\n",
    "    target = df[TARGET_COLS].fillna(-1).values.flatten()\n",
    "    patient_ids = df[\"id\"]\n",
    "\n",
    "    iterative_imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    data_imputed = pd.DataFrame(iterative_imputer.fit_transform(data), columns=FEATURE_COLS)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = pd.DataFrame(scaler.fit_transform(data_imputed))\n",
    "\n",
    "\n",
    "    label_prop_model = LabelPropagation(kernel='knn', n_neighbors=5)\n",
    "    label_prop_model.fit(data_scaled, target)\n",
    "    target_imputed = pd.DataFrame(label_prop_model.transduction_)\n",
    "\n",
    "    final_data = pd.concat([patient_ids, data_scaled, target_imputed], axis=1)\n",
    "    final_data.columns = ['id'] + FEATURE_COLS + [\"sii\"]\n",
    "    \n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba30041-fd8a-4e8d-858e-7f43385a3eb3",
   "metadata": {},
   "source": [
    "### Transformer Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25ab1ff-8cfb-4550-a8e7-caf3d9ccabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, chunk_size=1000):\n",
    "        self.sequences = sequences\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        # Convert to tensor and ensure float32\n",
    "        sequence = torch.FloatTensor(sequence)\n",
    "        \n",
    "        # If sequence is longer than chunk_size, average pool it\n",
    "        if len(sequence) > self.chunk_size:\n",
    "            # Reshape to (channels, length) for avg_pool1d\n",
    "            sequence = sequence.transpose(0, 1).unsqueeze(0)\n",
    "            target_length = self.chunk_size\n",
    "            kernel_size = len(sequence[0, 0]) // target_length\n",
    "            if kernel_size > 1:\n",
    "                sequence = nn.functional.avg_pool1d(sequence, kernel_size)\n",
    "            sequence = sequence.squeeze(0).transpose(0, 1)\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Sort sequences by length in descending order\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    # Get lengths of each sequence in the batch\n",
    "    lengths = torch.LongTensor([len(x) for x in batch])\n",
    "    \n",
    "    # Pad sequences to the same length\n",
    "    padded_seqs = rnn_utils.pad_sequence(batch, batch_first=True)\n",
    "    \n",
    "    return padded_seqs, lengths\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=8, d_model=64, nhead=8, num_layers=3, dim_feedforward=256, max_sequence_length=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Add initial downsampling if needed\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # x shape: (batch_size, max_seq_length, input_dim)\n",
    "        x = self.input_projection(x)\n",
    "        x = self.downsample(x)\n",
    "        \n",
    "        # Create padding mask for transformer\n",
    "        max_len = x.size(1)\n",
    "        device = x.device\n",
    "        arange_tensor = torch.arange(max_len, device=device)[None, :]\n",
    "        lengths_tensor = lengths[:, None].to(device)\n",
    "        padding_mask = arange_tensor >= lengths_tensor\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global pooling across sequence length\n",
    "        x = x.transpose(1, 2)  # (batch_size, d_model, max_seq_length)\n",
    "        x = self.global_pool(x)  # (batch_size, d_model, 1)\n",
    "        x = x.squeeze(-1)  # (batch_size, d_model)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        # Normalize embeddings\n",
    "        embeddings_normalized = nn.functional.normalize(embeddings, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(embeddings_normalized, embeddings_normalized.T)\n",
    "        \n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(similarity_matrix.size(0), device=similarity_matrix.device)\n",
    "        mask = 1 - mask\n",
    "        \n",
    "        # Scale similarities by temperature\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "        \n",
    "        # Compute loss\n",
    "        similarity_matrix = similarity_matrix * mask\n",
    "        positives = similarity_matrix.exp().sum(dim=1)\n",
    "        negatives = mask.sum(dim=1)\n",
    "        \n",
    "        loss = -torch.log(positives / negatives).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def train_transformer(all_participant_sequences, device='cuda' if torch.cuda.is_available() else 'cpu', chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Train the transformer model on all participant sequences\n",
    "    \"\"\"\n",
    "    # Create dataset and dataloader with custom collate function\n",
    "    dataset = TimeSeriesDataset(all_participant_sequences, chunk_size=chunk_size)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=16,  # Reduced batch size\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize model with smaller sequence length\n",
    "    model = TimeSeriesTransformer(max_sequence_length=chunk_size).to(device)\n",
    "    \n",
    "    # Initialize optimizer with gradient clipping\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Use contrastive loss instead of reconstruction\n",
    "    criterion = ContrastiveLoss(temperature=0.5).to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch, lengths in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get embeddings\n",
    "            embeddings = model(batch, lengths)\n",
    "            \n",
    "            # Compute contrastive loss\n",
    "            loss = criterion(embeddings)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_sequence_data(df):\n",
    "    \"\"\"\n",
    "    Prepare the sequence data by selecting relevant features and normalizing\n",
    "    \"\"\"\n",
    "    # Select features for the sequence\n",
    "    features = ['X', 'Y', 'Z', 'enmo', 'anglez', 'non-wear_flag', 'light', 'battery_voltage']\n",
    "    sequence_data = df[features].values\n",
    "    \n",
    "    # Normalize the data\n",
    "    mean = np.mean(sequence_data, axis=0)\n",
    "    std = np.std(sequence_data, axis=0)\n",
    "    normalized_data = (sequence_data - mean) / (std + 1e-8)\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea1fc6-391b-4aad-934d-3078eead571b",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35133c0c-c7ce-442e-a8f1-eb085f7b3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:45<00:00, 15.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "parquets = os.listdir('./dataset/series_train.parquet')\n",
    "parquets = [f'./dataset/series_train.parquet/{parquet}/part-0.parquet' for parquet in parquets]\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for i in tqdm(range(700)):\n",
    "    parquet_file = parquets[i]\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    sequence = prepare_sequence_data(df)\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5152c8-348a-48b2-9703-2d4e5c117253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, sequence, device='cuda', chunk_size=1000):\n",
    "    # Make sure model is in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a single sequence dataset\n",
    "    dataset = TimeSeriesDataset([sequence], chunk_size=chunk_size)\n",
    "    # Get the first (and only) item\n",
    "    processed_sequence = dataset[0]\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    sequence_tensor = processed_sequence.unsqueeze(0).to(device)  # Shape: [1, seq_length, features]\n",
    "    lengths = torch.LongTensor([len(processed_sequence)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        embedding = model(sequence_tensor, lengths)\n",
    "        \n",
    "    return embedding.cpu().numpy()  # Return as numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8be0e023-185d-485d-bbfa-1b07e39723ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[[ 1.2072891  -1.285907    1.0724608  -0.9647128  -1.1721855   0.59914154\n",
      "  -1.1655712  -1.1702367   1.3399656   0.8503978   0.36611465  1.1322119\n",
      "  -1.1436546   0.60717887  0.41804457  0.4304364   1.7853976  -0.7040854\n",
      "  -0.45579344  1.3113045  -0.16199487 -0.21923763 -1.5629803   1.6914625\n",
      "   0.0046412  -1.1569494   0.810713   -0.50895745 -1.0304841   1.108252\n",
      "  -0.22500947  0.6542294  -0.70435965  1.6573403  -0.26517263  1.130759\n",
      "   1.4824502   0.57356733 -0.8271706  -1.5393142   0.59242177 -1.2127146\n",
      "  -0.74388164  0.22022718 -2.1491644   0.18578534 -0.7996799   0.9271483\n",
      "  -0.8307708  -0.06390779 -1.1665395   1.3903908  -1.030352    1.4521902\n",
      "   1.0732347   0.6108448  -1.2699958  -0.68777955  0.7992131  -1.3558935\n",
      "  -0.02727532 -1.1247472   1.018531    0.22591838]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "checkpoint = torch.load('transformer_checkpoint.pth')\n",
    "model = TimeSeriesTransformer(\n",
    "    input_dim=checkpoint['input_dim'],\n",
    "    d_model=checkpoint['d_model']\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "single_sequence = sequences[0]\n",
    "embedding = get_embedding(model, single_sequence)\n",
    "\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8051fa97-e01f-4222-a556-3dbe084cfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def add_time_series_features(static_features):\n",
    "    all_features = []\n",
    "    \n",
    "    # Assuming static_features is a DataFrame and get_embedding returns a numpy array\n",
    "    for i in tqdm(range(len(sequences))):  # or use the full length of your dataset\n",
    "        # Get the embedding (assuming it is a numpy array with shape [1, 64])\n",
    "        embedding = get_embedding(model, sequences[i])\n",
    "        \n",
    "        # Convert embedding to a PyTorch tensor and flatten to shape [64]\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32).squeeze()  # Shape [64]\n",
    "        \n",
    "        # Select the corresponding row from static_features and convert to tensor\n",
    "        selected_feature = torch.tensor(static_features.iloc[i].values, dtype=torch.float32)  # Shape [48]\n",
    "        \n",
    "        # Concatenate the selected feature (48) and embedding (64) along dim=0\n",
    "        new_feature = torch.cat((selected_feature, embedding), dim=0)  # Shape [112]\n",
    "        \n",
    "        # Append the new feature and corresponding label to the lists\n",
    "        all_features.append(new_feature)\n",
    "        \n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    features_tensor = torch.stack(all_features)  # Shape [N, 112]\n",
    "    return features_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255b336b-8c53-4b87-acf0-6955d64909a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:24<00:00, 28.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for models with extra features: 1.0\n",
      "Accuracy for models without extra features: 0.9993865030674847\n",
      "Overall accuracy: 0.9994949494949495\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = './dataset/series_train.parquet'\n",
    "# Example: List of IDs with extra features\n",
    "ids_with_extra_features  = [name.split('=')[1] for name in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, name)) and name.startswith('id=')][:700]\n",
    "\n",
    "train_cross_sectional = data_pre_processing(\"./dataset/train.csv\")\n",
    "# Assuming train_cross_sectional is your DataFrame with 'id' and 'label' columns\n",
    "train_with_extra_features = train_cross_sectional[train_cross_sectional['id'].isin(ids_with_extra_features)]\n",
    "train_without_extra_features = train_cross_sectional[~train_cross_sectional['id'].isin(ids_with_extra_features)]\n",
    "\n",
    "# Separate features and labels\n",
    "X_with_extra = train_with_extra_features.drop(columns=['id', 'sii'])\n",
    "y_with_extra = train_with_extra_features['sii']\n",
    "X_with_extra = add_time_series_features(X_with_extra)\n",
    "\n",
    "X_without_extra = train_without_extra_features.drop(columns=['id', 'sii'])\n",
    "y_without_extra = train_without_extra_features['sii']\n",
    "\n",
    "# Initialize XGBoost classifiers\n",
    "model_with_extra = xgb.XGBClassifier()\n",
    "model_without_extra = xgb.XGBClassifier()\n",
    "\n",
    "# Train the models\n",
    "model_with_extra.fit(X_with_extra, y_with_extra)\n",
    "model_without_extra.fit(X_without_extra, y_without_extra)\n",
    "\n",
    "# Predict on the training data (or use separate test sets)\n",
    "y_pred_with_extra = model_with_extra.predict(X_with_extra)\n",
    "y_pred_without_extra = model_without_extra.predict(X_without_extra)\n",
    "\n",
    "# Calculate accuracy for each model\n",
    "acc_with_extra = accuracy_score(y_with_extra, y_pred_with_extra)\n",
    "acc_without_extra = accuracy_score(y_without_extra, y_pred_without_extra)\n",
    "\n",
    "print(f\"Accuracy for models with extra features: {acc_with_extra}\")\n",
    "print(f\"Accuracy for models without extra features: {acc_without_extra}\")\n",
    "\n",
    "# Combine predictions for overall accuracy\n",
    "y_pred_combined = y_pred_with_extra.tolist() + y_pred_without_extra.tolist()\n",
    "y_true_combined = y_with_extra.tolist() + y_without_extra.tolist()\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
    "\n",
    "print(f\"Overall accuracy: {overall_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73a6d2-84b0-4981-ba99-a1e91308f9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
